# train_model.py (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)

from transformers import (
    T5ForConditionalGeneration,
    T5Tokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq
)
from prepare_dataset import prepare_dataset_for_t5
import torch

# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
model_name = "cointegrated/rut5-base-multitask"  # –†—É—Å—Å–∫–∞—è T5
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_name}")

# 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
print("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
dataset = prepare_dataset_for_t5()


# 3. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
def tokenize_function(examples):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ —Ç–æ–∫–µ–Ω—ã"""
    model_inputs = tokenizer(
        examples['input_text'],
        max_length=512,
        truncation=True,
        padding='max_length'
    )

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ü–µ–ª–µ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
    labels = tokenizer(
        examples['target_text'],
        max_length=128,
        truncation=True,
        padding='max_length'
    )

    model_inputs['labels'] = labels['input_ids']
    return model_inputs


print("–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è...")
tokenized_dataset = dataset.map(tokenize_function, batched=True)

# 4. Data Collator
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model
)

# 5. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è (‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û)
training_args = TrainingArguments(
    output_dir="./fine_tuned_model",  # –ì–¥–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å
    eval_strategy="epoch",  # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –±—ã–ª–æ evaluation_strategy
    learning_rate=3e-4,  # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
    per_device_train_batch_size=4,  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
    per_device_eval_batch_size=4,
    num_train_epochs=3,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
    weight_decay=0.01,  # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    save_total_limit=2,  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ç–æ–ª—å–∫–æ 2 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–∞
    logging_dir="./logs",
    logging_steps=50,
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    fp16=False,  # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: False –¥–ª—è CPU
    report_to="none"  # –û—Ç–∫–ª—é—á–∏—Ç—å wandb/tensorboard
)

# 6. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator
)

# 7. –û–ë–£–ß–ï–ù–ò–ï
print("\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
trainer.train()

# 8. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
print("\nüìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...")
eval_results = trainer.evaluate()
print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {eval_results}")

# 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
model.save_pretrained("./fine_tuned_model/best_model")
tokenizer.save_pretrained("./fine_tuned_model/best_model")

print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ ./fine_tuned_model/best_model")
