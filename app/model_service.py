from transformers import T5ForConditionalGeneration, T5Tokenizer
import pdfplumber
import torch
import os


class QAGenerator:
    def __init__(self, model_path: str = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –≤–æ–ø—Ä–æ—Å–æ–≤

        Args:
            model_path: –ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤–∞—è)
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        if model_path and os.path.exists(model_path):
            print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏–∑ {model_path}")
            self.tokenizer = T5Tokenizer.from_pretrained(model_path)
            self.model = T5ForConditionalGeneration.from_pretrained(model_path).to(self.device)
            print("‚úÖ –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        else:
            print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
            model_name = "cointegrated/rut5-base-multitask"
            self.tokenizer = T5Tokenizer.from_pretrained(model_name)
            self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(self.device)
            print("‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    def extract_pdf_text(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF"""
        text = ''
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + '\n'
        return text

    def generate_question(self, context: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        input_text = f"generate question: {context}"

        inputs = self.tokenizer(
            input_text,
            return_tensors="pt",
            max_length=512,
            truncation=True
        ).to(self.device)

        outputs = self.model.generate(
            **inputs,
            max_length=64,
            num_beams=4,
            early_stopping=True,
            temperature=0.7
        )

        question = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return question

    def process_pdf(self, file_path: str, max_cards: int = 10) -> list:
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥: PDF ‚Üí –∫–∞—Ä—Ç–æ—á–∫–∏"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
        text = self.extract_pdf_text(file_path)

        if not text.strip():
            return []

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏
        chunk_size = 500
        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏
        cards = []
        for idx, chunk in enumerate(chunks[:max_cards]):
            try:
                question = self.generate_question(chunk)
                cards.append({
                    'id': idx,
                    'question': question,
                    'answer': chunk[:200],  # –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ –æ—Ç–≤–µ—Ç
                    'source': chunk[:50]
                })
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞—Ä—Ç–æ—á–∫–∏ {idx}: {e}")
                continue

        return cards
