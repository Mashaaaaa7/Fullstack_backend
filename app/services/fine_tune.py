"""
Fine-tuning —Å–∫—Ä–∏–ø—Ç –¥–ª—è Question Generation –º–æ–¥–µ–ª–∏ –Ω–∞ SberQuAD –¥–∞—Ç–∞—Å–µ—Ç–µ
–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ - —Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å–æ —Å—Ç–∞—Ä—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ transformers
"""

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, DataCollatorForSeq2Seq
import torch

'''
–ü—Ä–∏–º–µ—Ä: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ 4 –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ 4 –ø—Ä–∏–º–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ 
–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å–ª–µ–¥—É—é—â–µ–π –ø–∞—Ä—Ç–∏–∏ –∏–∑ 4 –ø—Ä–∏–º–µ—Ä–æ–≤. 
'''
# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
MODEL_NAME = "google/mt5-small"
DATASET_NAME = "kuznetsoffandrey/sberquad"
OUTPUT_DIR = "./models/qg-finetuned"
BATCH_SIZE = 4
EPOCHS = 1
MAX_INPUT_LENGTH = 512  # –î–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
MAX_TARGET_LENGTH = 100  # –î–ª–∏–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã


print("üì• –ó–∞–≥—Ä—É–∂–∞—é SberQuAD –¥–∞—Ç–∞—Å–µ—Ç...")
dataset = load_dataset(DATASET_NAME)

print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(dataset['train'])} –ø—Ä–∏–º–µ—Ä–æ–≤")

# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä
example = dataset['train'][0]
print(f"\n–ü—Ä–∏–º–µ—Ä –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
print(f"Context: {example['context'][:200]}...")
print(f"Question: {example['question']}")
print(f"Answer: {example['answers']['text'][0]}")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
print("\nüîß –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä...")
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
except Exception as e:
    print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {MODEL_NAME}: {e}")
    print("–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å fallback –º–æ–¥–µ–ª—å...")
    MODEL_NAME = "t5-small"
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º: {device}")
print(f"–ú–æ–¥–µ–ª—å: {MODEL_NAME}")

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
def preprocess_function(examples):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º SberQuAD –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Question Generation"""

    inputs = []
    targets = []

    for i in range(len(examples['context'])):
        context = examples['context'][i]
        question = examples['question'][i]

        input_text = context

        inputs.append(input_text)
        targets.append(question)

    # Tokenize
    model_inputs = tokenizer(
        inputs,
        max_length=MAX_INPUT_LENGTH,
        truncation=True,
        padding="max_length"
    )

    labels = tokenizer(
        targets,
        max_length=MAX_TARGET_LENGTH,
        truncation=True,
        padding="max_length"
    )

    model_inputs["labels"] = labels["input_ids"]

    return model_inputs

print("\n‚öôÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
train_size = min(5000, len(dataset['train']))  # –ï—â–µ –º–µ–Ω—å—à–µ –¥–ª—è —Å—Ç–∞—Ä–æ–π –≤–µ—Ä—Å–∏–∏
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º {train_size} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")

train_dataset = dataset['train'].select(range(train_size))
val_size = min(500, len(dataset['validation']))
eval_dataset = dataset['validation'].select(range(val_size))

processed_train = train_dataset.map(
    preprocess_function,
    batched=True,
    batch_size=500,
    remove_columns=train_dataset.column_names,
    desc="Processing train"
)

processed_eval = eval_dataset.map(
    preprocess_function,
    batched=True,
    batch_size=500,
    remove_columns=eval_dataset.column_names,
    desc="Processing eval"
)

print(f"Train: {len(processed_train)}, Eval: {len(processed_eval)}")

# Training arguments - —Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å–æ –û–ß–ï–ù–¨ —Å—Ç–∞—Ä—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    num_train_epochs=EPOCHS,
    weight_decay=0.01,
    save_total_limit=1,
    logging_steps=100,
    save_steps=500,
    warmup_steps=100,
    report_to="none"
)

# Data collator
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=processed_train,
    eval_dataset=processed_eval,
    data_collator=data_collator,
)

print("\nüöÄ –ù–∞—á–∏–Ω–∞—é fine-tuning...")
print(f"üìä –ë–∞—Ç—á: {BATCH_SIZE}, –≠–ø–æ—Ö–∏: {EPOCHS}, –ü—Ä–∏–º–µ—Ä–æ–≤: {len(processed_train)}")
print(f"üíæ –ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {OUTPUT_DIR}")

trainer.train()

print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {OUTPUT_DIR}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print("‚úì –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
print("\nüìù qa_generator.py –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å")