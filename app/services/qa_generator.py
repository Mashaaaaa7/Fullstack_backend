import re
import unicodedata
from typing import List, Dict
from transformers import pipeline
import pdfplumber
import torch


class QAGenerator:
    def __init__(self, use_gpt: bool = False, model_name: str = "cointegrated/rut5-base-multitask"):
        self.device = 0 if torch.cuda.is_available() else -1
        self.use_gpt = use_gpt
        print("‚è≥ –ó–∞–≥—Ä—É–∂–∞—é —Ä—É—Å—Å–∫—É—é –º–æ–¥–µ–ª—å...")
        self.generator = pipeline(
            "text2text-generation",
            model="cointegrated/rut5-base-multitask",
            device=self.device,
            torch_dtype=torch.float32
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    def clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
        if not text:
            return ""
        text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C' or ch in '\n\t')
        text = re.sub(r'[>~<‚Ä¢¬ª¬´‚Äû"\[\]{}()_\-‚Äì‚Äî]+', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def extract_meaningful_text(self, file_path: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã"""
        chunks = []
        try:
            with pdfplumber.open(file_path) as pdf:
                print(f"üìÑ PDF –∏–º–µ–µ—Ç {len(pdf.pages)} —Å—Ç—Ä–∞–Ω–∏—Ü")

                for i, page in enumerate(pdf.pages):
                    raw_text = page.extract_text()
                    if not raw_text:
                        continue

                    text = self.clean_text(raw_text)
                    if len(text) < 100:
                        continue

                    text = re.sub(r'^\d{2}\.\d{2}\.\d{4}.*?Colab\s*', '', text)
                    text = re.sub(r'https?://[^\s]+', '', text)
                    text = re.sub(r'\d{4}.*?ipynb.*?Colab', '', text, flags=re.IGNORECASE)

                    paragraphs = [p.strip() for p in text.split('\n') if len(p.strip()) > 50]

                    for para in paragraphs:
                        chunks_from_para = self._split_into_chunks(para)
                        chunks.extend(chunks_from_para)

            chunks = [c for c in chunks if not any(
                bad in c['text'].lower() for bad in ['ipynb', 'colab', 'http', '¬©', '¬Æ']
            )]

            print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(chunks)} —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            return chunks
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return []

    def _split_into_chunks(self, text: str) -> List[Dict]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ –∫—É—Å–∫–∏"""
        chunks = []
        sentences = re.split(r'[.!?]+\s+', text)

        combined = []
        current = ""

        for sent in sentences:
            sent = sent.strip()
            if not sent or len(sent) < 5:
                continue

            current += sent + ". "

            if len(current.split()) >= 12:
                combined.append(current.strip())
                current = ""

        if current.strip():
            combined.append(current.strip())

        for chunk_text in combined:
            if len(chunk_text) > 60:
                chunks.append({
                    "text": chunk_text,
                    "page": 0,
                    "word_count": len(chunk_text.split())
                })

        return chunks

    def _clean_question(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –æ—Ç –º—É—Å–æ—Ä–∞"""
        text = re.sub(r'^–Ω–∞–ø–∏—à–∏—Ç–µ –≤–æ–ø—Ä–æ—Å.*?:\s*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'^–≤–æ–ø—Ä–æ—Å.*?:\s*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'^–Ω–∞ –æ—Å–Ω–æ–≤–µ.*?:\s*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'^—Å–æ–∑–¥–∞–π—Ç–µ.*?:\s*', '', text, flags=re.IGNORECASE)

        text = text.rstrip('.,;:')

        if text:
            text = text[0].upper() + text[1:].lower()

        if text and not text.endswith('?'):
            text += '?'

        return text.strip()

    def _generate_question_rut5(self, answer: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å —á–µ—Ä–µ–∑ RuT5"""
        try:
            text_sample = answer[:250]
            prompt = f"–°–æ–∑–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –∫ —Ç–µ–∫—Å—Ç—É: {text_sample}"

            result = self.generator(
                prompt,
                max_new_tokens=40,
                num_beams=3,
                temperature=0.6
            )

            question = self.clean_text(result[0]['generated_text']).strip()
            question = self._clean_question(question)

            if (15 < len(question) < 120 and '?' in question and
                    not question.lower().startswith('–Ω–∞–ø–∏—à–∏—Ç–µ') and
                    not question.lower().startswith('—Å–æ–∑–¥–∞–π—Ç–µ')):
                return question

            return None
        except Exception as e:
            print(f"‚ö†Ô∏è RuT5 –æ—à–∏–±–∫–∞: {e}")
            return None

    def _generate_universal_question(self, answer: str) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π fallback"""
        words = answer.split()
        answer_lower = answer.lower()

        bad_words = {
            '—ç—Ç–æ', '–¥–ª—è', '–ø—Ä–∏', '–∫–∞–∫', '—á—Ç–æ', '–≤', '–ø–æ', '–Ω–∞', '—Å', '–∏', '–∏–ª–∏', '—Ç–æ',
            '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–∏', '–±—ã—Ç—å', '—è–≤–ª—è—é—Ç—Å—è', '—è–≤–ª—è–µ—Ç—Å—è', '–µ—Å—Ç—å', '–∏–º–µ–ª–∏',
            '–∏–º–µ—é—Ç', '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è', '–Ω–∞—Ö–æ–¥–∏–ª–∏—Å—å', '–≤–∞–∂–Ω—ã–π', '–≤–∞–∂–Ω–∞—è', '–≥–ª–∞–≤–Ω—ã–π', '–Ω–æ–≤—ã–π',
            '–ø—Ä–æ—Ü–µ—Å—Å', '–≤–µ–ª–∏–∫–æ–≥–æ', '–Ω–∞–ø—Ä–∏–º–µ—Ä', '–Ω–µ—Å–º–æ—Ç—Ä—è', '–≥–ª–∞–≤–Ω—ã–µ', '–º–µ—Å—Ç–Ω–æ–µ',
            '–≤–ª–∏—è–Ω–∏–µ', '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–æ–µ', '—É–µ–∑–¥—ã', '–µ–∫–∞—Ç–µ—Ä–∏–Ω—ã', '—Å–∏—Å—Ç–µ–º–∞', '—Ä–µ—Ñ–æ—Ä–º–∞'
        }

        idx = 0
        while idx < len(words) and words[idx].lower() in bad_words:
            idx += 1

        remaining_words = words[idx:]

        key_phrase = None
        for w in remaining_words[:15]:
            w_lower = w.lower().rstrip(',:;.')
            if (len(w_lower) > 5 and w[0].isupper() and w_lower not in bad_words):
                key_phrase = w_lower
                break

        if not key_phrase:
            return None

        if any(word in answer_lower for word in ['–æ–∫–∞–∑–∞–ª–∞', '–ø—Ä–∏–≤–µ–ª', '–≤—ã–∑–≤–∞']):
            return f"–ö–∞–∫–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ –∏–º–µ–ª {key_phrase}?"
        elif any(word in answer_lower for word in ['—Ä–∞–∑–≤–∏–≤', '—ç–≤–æ–ª—é—Ü', '–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤']):
            return f"–ö–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏–ª–æ —Ä–∞–∑–≤–∏—Ç–∏–µ {key_phrase}?"
        elif any(word in answer_lower for word in ['–ø—Ä–∏–≤–µ–ª–∞', '–ø–æ—Å–ª—É–∂–∏–ª–∞', '—Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–≤–∞']):
            return f"–ö–∞–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã —Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–≤–∞–ª–∏ {key_phrase}?"
        elif any(word in answer_lower for word in ['–∏–≥—Ä–∞–ª–∞', '–≤—ã–ø–æ–ª–Ω—è', '—Å–ª—É–∂–∏–ª–∞', '—Ä–æ–ª—å']):
            return f"–ö–∞–∫—É—é —Ä–æ–ª—å –≤—ã–ø–æ–ª–Ω—è–ª {key_phrase}?"
        elif any(word in answer_lower for word in ['—Å–æ–¥–µ—Ä–∂–∏—Ç', '–≤–∫–ª—é—á–∞–µ—Ç']):
            return f"–ò–∑ —á–µ–≥–æ —Å–æ—Å—Ç–æ–∏—Ç {key_phrase}?"
        elif any(word in answer_lower for word in ['–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç', '—è–≤–ª–µ–Ω–∏–µ–º']):
            return f"–ß—Ç–æ —Ç–∞–∫–æ–µ {key_phrase}?"
        elif any(word in answer_lower for word in ['–≤–≤–µ–ª', '–≤–≤–µ–¥–µ–Ω', '–ø–æ–¥–ø–∏—Å–∞—Ç—å']):
            return f"–ß—Ç–æ —Å–¥–µ–ª–∞–ª {key_phrase}?"

        return None

    def _is_corrupted_text(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–µ –ø–æ–≤—Ä–µ–∂–¥—ë–Ω –ª–∏ —Ç–µ–∫—Å—Ç"""
        if any(pattern in text for pattern in [
            'znp', 'Zogitp', 'modelnp', 'zn√†', 's√†', '—Ä—É=–æ', 'n–ær–∏—Å—Ç–∏—á–µ—Å–∫–æ–π'
        ]):
            return True

        if text.count('=') > 2 or text.count('?') > 1:
            return True

        if re.search(r'[–∞-—è–ê-–Ø][a-zA-Z]|[a-zA-Z][–∞-—è–ê-–Ø]', text):
            return True

        return False

    def _is_valid_question(self, question: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –≤–æ–ø—Ä–æ—Å–∞"""
        if not question or not question.endswith('?'):
            return False

        if len(question) < 12 or len(question) > 150:
            return False

        words = question.split()
        if len(words) < 3:
            return False

        # ‚ùå –ù–û–í–´–ô –§–ò–õ–¨–¢–†: –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        # –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å "–í ", "–ò–∑ ", "–ù–∞ " –∏ –¥–∞–ª—å—à–µ –∏–¥—ë—Ç –º–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ + "?"
        # —ç—Ç–æ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ, –∞ –Ω–µ –≤–æ–ø—Ä–æ—Å

        bad_patterns = [
            r'^–≤ –æ–±–º–µ–Ω –Ω–∞.*\?$',  # "–í –æ–±–º–µ–Ω –Ω–∞..."
            r'^–≤ –ø–µ—Ä–≤—ã–µ.*\?$',  # "–í –ø–µ—Ä–≤—ã–µ –≥–æ–¥–∞..."
            r'^–∏–∑.*\?$',  # "–ò–∑ –¥–∞—Ç–æ—á–Ω—ã—Ö..."
            r'^–Ω–∞.*\?$',  # "–ù–∞ –∑–∏–º—É..."
        ]

        for pattern in bad_patterns:
            if re.search(pattern, question.lower()):
                return False

        # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–´–ï –≤–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å:
        good_starts = ['—á—Ç–æ', '–∫–∞–∫', '–∫–∞–∫–æ–π', '–∫–∞–∫–∏–µ', '–∫—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '—á–µ–º', '–∏–∑ —á–µ–≥–æ']

        first_word = words[0].lower().rstrip('?,.:;')
        if not first_word in good_starts:
            # –ú–æ–∂–µ—Ç –±—ã—Ç—å —ç—Ç–æ –Ω–µ-—Ä—É—Å—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            return False

        # –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        if re.search(r'—á—Ç–æ —Å–¥–µ–ª–∞–ª[–∞]? (—É–ø—Ä–∞–≤–ª–µ–Ω–∏–π?|–ø–µ—Ä–∏–æ–¥|—Å–∏—Å—Ç–µ–º–∞|—Ä–µ–≤–æ–ª—é—Ü)\?', question, re.IGNORECASE):
            return False

        if '–æ–∫–∞–∑–∞–ª' in question.lower() and '–ø–µ—Ä–∏–æ–¥' in question.lower():
            return False

        return True

    def generate_qa_pair(self, context: str) -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç QA –ø–∞—Ä—É —Å –ø–æ–ª–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        try:
            context_clean = self.clean_text(context[:700])
            context_clean = re.sub(r'\s+', ' ', context_clean).strip()

            if len(context_clean) < 120:
                return None

            if self._is_corrupted_text(context_clean):
                return None

            if any(word in context_clean.lower() for word in ['–∫–æ–¥', 'import', 'def ']):
                return None

            sentences = [s.strip() for s in re.split(r'[.!?]+', context_clean)]
            candidate_sents = [s for s in sentences if len(s.split()) >= 12 and len(s) > 100]

            if not candidate_sents:
                return None

            answer = candidate_sents[0]

            question = self._generate_question_rut5(answer)

            if not question:
                question = self._generate_universal_question(answer)

            if not question or not self._is_valid_question(question):
                return None

            answer = re.sub(r'\s+', ' ', answer).strip()
            question = re.sub(r'\s+', ' ', question).strip()

            if len(question) > 15 and len(answer) > 100:
                return {
                    "question": question,
                    "answer": answer,
                    "context": context_clean[:150]
                }

            return None

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
            return None

    def process_pdf(self, file_path: str, max_cards: int = 10) -> List[Dict]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç PDF –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–∞—Ä—Ç–æ—á–∫–∏"""
        print(f"\nüîÑ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {file_path}...")
        print(f"üéØ –¶–µ–ª—å: {max_cards} –∫–∞—Ä—Ç–æ—á–µ–∫")

        chunks = self.extract_meaningful_text(file_path)

        if not chunks:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤!")
            return []

        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(chunks)} —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")

        chunks.sort(key=lambda x: abs(x['word_count'] - 25))
        flashcards = []

        for chunk in chunks[:max_cards * 2]:
            if len(flashcards) >= max_cards:
                break

            qa_pair = self.generate_qa_pair(chunk['text'])

            if qa_pair:
                flashcard = {
                    "id": len(flashcards) + 1,
                    "question": qa_pair["question"],
                    "answer": qa_pair["answer"],
                    "context": qa_pair["context"],
                    "source": f"Page {chunk['page']}"
                }
                flashcards.append(flashcard)
                print(f"  ‚úÖ [{len(flashcards)}] {qa_pair['question'][:60]}...")

        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(flashcards)} –∫–∞—Ä—Ç–æ—á–µ–∫")
        return flashcards
