import re
import unicodedata
from typing import List, Dict
from transformers import pipeline
import pdfplumber
import torch

class QAGenerator:
    def __init__(self, use_gpt: bool = False, model_name: str = "cointegrated/rut5-base-multitask"):
        # Device
        self.device = 0 if torch.cuda.is_available() else -1
        self.use_gpt = use_gpt
        print("‚è≥ –ó–∞–≥—Ä—É–∂–∞—é —Ä—É—Å—Å–∫—É—é –º–æ–¥–µ–ª—å...")
        self.generator = pipeline(
            "text2text-generation",
            model="cointegrated/rut5-base-multitask",
            device=self.device,
            torch_dtype=torch.float32
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    def clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
        if not text:
            return ""
        text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C' or ch in '\n\t')
        text = re.sub(r'[>~<‚Ä¢¬ª¬´‚Äû"\[\]{}()_\-‚Äì‚Äî]+', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def extract_meaningful_text(self, file_path: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã"""
        chunks = []
        try:
            with pdfplumber.open(file_path) as pdf:
                print(f"üìÑ PDF –∏–º–µ–µ—Ç {len(pdf.pages)} —Å—Ç—Ä–∞–Ω–∏—Ü")

                for i, page in enumerate(pdf.pages):
                    raw_text = page.extract_text()
                    if not raw_text:
                        continue

                    text = self.clean_text(raw_text)
                    if len(text) < 100:
                        continue

                    text = re.sub(r'^\d{2}\.\d{2}\.\d{4}.*?Colab\s*', '', text)
                    text = re.sub(r'https?://[^\s]+', '', text)
                    text = re.sub(r'\d{4}.*?ipynb.*?Colab', '', text, flags=re.IGNORECASE)

                    paragraphs = [p.strip() for p in text.split('\n') if len(p.strip()) > 50]

                    for para in paragraphs:
                        chunks_from_para = self._split_into_chunks(para)
                        chunks.extend(chunks_from_para)

            chunks = [c for c in chunks if not any(
                bad in c['text'].lower() for bad in ['ipynb', 'colab', 'http', '¬©', '¬Æ']
            )]

            print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(chunks)} —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            return chunks
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return []

    def _split_into_chunks(self, text: str) -> List[Dict]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ –∫—É—Å–∫–∏"""
        chunks = []
        sentences = re.split(r'[.!?]+\s+', text)

        combined = []
        current = ""

        for sent in sentences:
            sent = sent.strip()
            if not sent or len(sent) < 5:
                continue

            current += sent + ". "

            if len(current.split()) >= 12:
                combined.append(current.strip())
                current = ""

        if current.strip():
            combined.append(current.strip())

        for chunk_text in combined:
            if len(chunk_text) > 60:
                chunks.append({
                    "text": chunk_text,
                    "page": 0,
                    "word_count": len(chunk_text.split())
                })

        return chunks

    def _clean_question(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –æ—Ç –º—É—Å–æ—Ä–∞"""
        # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–º–ø—Ç—ã –≤ –Ω–∞—á–∞–ª–µ
        text = re.sub(r'^–Ω–∞–ø–∏—à–∏—Ç–µ –≤–æ–ø—Ä–æ—Å.*?:\s*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'^–≤–æ–ø—Ä–æ—Å.*?:\s*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'^–Ω–∞ –æ—Å–Ω–æ–≤–µ.*?:\s*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'^—Å–æ–∑–¥–∞–π—Ç–µ.*?:\s*', '', text, flags=re.IGNORECASE)

        # –£–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä –≤ –∫–æ–Ω—Ü–µ
        text = text.rstrip('.,;:')

        # –ö–∞–ø–∏—Ç–∞–ª–∏–∑–∏—Ä—É–µ–º
        if text:
            text = text[0].upper() + text[1:].lower()

        # –î–æ–±–∞–≤–ª—è–µ–º ?
        if text and not text.endswith('?'):
            text += '?'

        return text.strip()

    def _generate_question_rut5(self, answer: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å —á–µ—Ä–µ–∑ RuT5"""
        try:
            text_sample = answer[:250]

            # –õ–£–ß–®–ò–ô –ü–†–û–ú–ü–¢
            prompt = f"–°–æ–∑–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –∫ —Ç–µ–∫—Å—Ç—É: {text_sample}"

            result = self.generator(
                prompt,
                max_new_tokens=40,
                num_beams=3,
                temperature=0.6
            )

            question = self.clean_text(result[0]['generated_text']).strip()
            question = self._clean_question(question)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            if (15 < len(question) < 120 and '?' in question and
                    not question.lower().startswith('–Ω–∞–ø–∏—à–∏—Ç–µ') and
                    not question.lower().startswith('—Å–æ–∑–¥–∞–π—Ç–µ')):
                return question

            return None
        except Exception as e:
            print(f"‚ö†Ô∏è RuT5 –æ—à–∏–±–∫–∞: {e}")
            return None

    def _generate_universal_question(self, answer: str) -> str:
        """Fallback: —à–∞–±–ª–æ–Ω—ã –≤–æ–ø—Ä–æ—Å–æ–≤"""
        words = answer.split()
        answer_lower = answer.lower()

        bad_words = {'—ç—Ç–æ', '–¥–ª—è', '–ø—Ä–∏', '–∫–∞–∫', '—á—Ç–æ', '–≤', '–ø–æ', '–Ω–∞', '—Å', '–∏', '–∏–ª–∏', '—Ç–æ',
                     '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–∏', '–±—ã—Ç—å', '—è–≤–ª—è—é—Ç—Å—è', '—è–≤–ª—è–µ—Ç—Å—è', '–µ—Å—Ç—å', '–∏–º–µ–ª–∏',
                     '–∏–º–µ—é—Ç', '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è', '–Ω–∞—Ö–æ–¥–∏–ª–∏—Å—å', '–≤–∞–∂–Ω—ã–π', '–≤–∞–∂–Ω–∞—è', '–≥–ª–∞–≤–Ω—ã–π', '–Ω–æ–≤—ã–π'}

        idx = 0
        while idx < len(words) and words[idx].lower() in bad_words:
            idx += 1

        remaining_words = words[idx:]

        for w in remaining_words[:12]:
            w_lower = w.lower().rstrip(',:;.')
            if (len(w_lower) > 4 and w[0].isupper() and w_lower not in bad_words and
                    not w_lower.endswith('–æ–º') and not w_lower.endswith('—ã–π') and
                    not w_lower.endswith('–æ–π')):
                key_phrase = w_lower
                break
        else:
            key_phrase = "–ø—Ä–æ—Ü–µ—Å—Å"

        if any(word in answer_lower for word in ['–æ–∫–∞–∑–∞–ª–∞', '–ø—Ä–∏–≤–µ–ª', '–≤—ã–∑–≤–∞']):
            return f"–ö–∞–∫–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ –∏–º–µ–ª {key_phrase}?"
        elif any(word in answer_lower for word in ['—Ä–∞–∑–≤–∏–≤', '—ç–≤–æ–ª—é—Ü', '–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤']):
            return f"–ö–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏–ª–æ —Ä–∞–∑–≤–∏—Ç–∏–µ {key_phrase}?"
        elif any(word in answer_lower for word in ['–ø—Ä–∏–≤–µ–ª–∞', '–ø–æ—Å–ª—É–∂–∏–ª–∞', '—Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–≤–∞']):
            return f"–ö–∞–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã —Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–≤–∞–ª–∏ {key_phrase}?"
        elif any(word in answer_lower for word in ['–∏–≥—Ä–∞–ª–∞', '–≤—ã–ø–æ–ª–Ω—è', '—Å–ª—É–∂–∏–ª–∞']):
            return f"–ö–∞–∫—É—é —Ä–æ–ª—å –≤—ã–ø–æ–ª–Ω—è–ª {key_phrase}?"
        elif any(word in answer_lower for word in ['—Å–æ–¥–µ—Ä–∂–∏—Ç', '–≤–∫–ª—é—á–∞–µ—Ç']):
            return f"–ò–∑ —á–µ–≥–æ —Å–æ—Å—Ç–æ–∏—Ç {key_phrase}?"
        else:
            return f"–û–±—ä—è—Å–Ω–∏—Ç–µ, —á—Ç–æ —Ç–∞–∫–æ–µ {key_phrase}?"

    def _is_corrupted_text(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–µ –ø–æ–≤—Ä–µ–∂–¥—ë–Ω –ª–∏ —Ç–µ–∫—Å—Ç"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –º—É—Å–æ—Ä
        if any(pattern in text for pattern in [
            'znp', 'Zogitp', 'modelnp', 'zn√†', 's√†', '—Ä—É=–æ', 'n–ær–∏—Å—Ç–∏—á–µ—Å–∫–æ–π'
        ]):
            return True

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤ = –∏–ª–∏ ?
        if text.count('=') > 2 or text.count('?') > 1:
            return True

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü—É + –ª–∞—Ç–∏–Ω–∏—Ü—É –≤ –æ–¥–Ω–æ–º —Å–ª–æ–≤–µ
        if re.search(r'[–∞-—è–ê-–Ø][a-zA-Z]|[a-zA-Z][–∞-—è–ê-–Ø]', text):
            return True

        return False

    def generate_qa_pair(self, context: str) -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç QA —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –º—É—Å–æ—Ä–∞"""
        try:
            context_clean = self.clean_text(context[:700])
            context_clean = re.sub(r'\s+', ' ', context_clean).strip()

            if len(context_clean) < 120:
                return None

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–æ–≤—Ä–µ–∂–¥—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –ò–ó PDF
            if self._is_corrupted_text(context_clean):
                return None

            if any(word in context_clean.lower() for word in
                   ['–∫–æ–¥', 'import', 'def ', 'print(', 'function', 'class ']):
                return None

            sentences = [s.strip() for s in re.split(r'[.!?]+', context_clean)]
            candidate_sents = [s for s in sentences if len(s.split()) >= 12 and len(s) > 100]

            if not candidate_sents:
                return None

            answer = candidate_sents[0]

            question = self._generate_question_rut5(answer)

            # Fallback
            if not question:
                question = self._generate_universal_question(answer)

            if not question:
                return None

            answer = re.sub(r'\s+', ' ', answer).strip()
            question = re.sub(r'\s+', ' ', question).strip()

            if len(question) > 15 and len(answer) > 100:
                return {
                    "question": question,
                    "answer": answer,
                    "context": context_clean[:150]
                }

            return None

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
            return None

    def process_pdf(self, file_path: str, max_cards: int = 10) -> List[Dict]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç PDF –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–∞—Ä—Ç–æ—á–∫–∏"""
        print(f"\nüîÑ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {file_path}...")
        print(f"üéØ –¶–µ–ª—å: {max_cards} –∫–∞—Ä—Ç–æ—á–µ–∫")

        chunks = self.extract_meaningful_text(file_path)

        if not chunks:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤!")
            return []

        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(chunks)} —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")

        chunks.sort(key=lambda x: abs(x['word_count'] - 25))
        flashcards = []

        for chunk in chunks[:max_cards * 2]:
            if len(flashcards) >= max_cards:
                break

            qa_pair = self.generate_qa_pair(chunk['text'])

            if qa_pair:
                flashcard = {
                    "id": len(flashcards) + 1,
                    "question": qa_pair["question"],
                    "answer": qa_pair["answer"],
                    "context": qa_pair["context"],
                    "source": f"Page {chunk['page']}"
                }
                flashcards.append(flashcard)
                print(f"  ‚úÖ [{len(flashcards)}] {qa_pair['question'][:60]}...")

        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(flashcards)} –∫–∞—Ä—Ç–æ—á–µ–∫")
        return flashcards
