"""
QA Generator –¥–ª—è PDF –æ–±—Ä–∞–±–æ—Ç–∫–∏
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç fine-tuned T5 –º–æ–¥–µ–ª—å –Ω–∞ SberQuAD –¥–∞—Ç–∞—Å–µ—Ç–µ
–° –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–æ–π input –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º output
"""
import PyPDF2
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import logging
from typing import List, Dict, Tuple
import re
import nltk
from nltk.tokenize import sent_tokenize

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∂–∞–µ–º NLTK –¥–∞–Ω–Ω—ã–µ
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–µ–π
qg_model = None
qg_tokenizer = None


def load_qg_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç fine-tuned –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤"""
    global qg_model, qg_tokenizer
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º fine-tuned –º–æ–¥–µ–ª—å
        model_path = "./models/qg-finetuned"
        logger.info(f"üì• –ó–∞–≥—Ä—É–∂–∞—é fine-tuned –º–æ–¥–µ–ª—å –∏–∑ {model_path}...")

        qg_tokenizer = AutoTokenizer.from_pretrained(model_path)
        qg_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –Ω–∞ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        device = "cuda" if torch.cuda.is_available() else "cpu"
        qg_model.to(device)
        qg_model.eval()

        logger.info(f"‚úì Fine-tuned T5 –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device}")
        return True

    except Exception as e:
        logger.error(f"‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ fine-tuned –º–æ–¥–µ–ª–∏: {e}")
        return False


def extract_text_from_pdf(pdf_path: str) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF —Ñ–∞–π–ª–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º"""
    try:
        pages_text = []
        with open(pdf_path, 'rb') as f:
            pdf_reader = PyPDF2.PdfReader(f)
            for page_num, page in enumerate(pdf_reader.pages):
                try:
                    extracted = page.extract_text()
                    if extracted and extracted.strip():
                        pages_text.append(extracted)
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
                    continue

        if not pages_text:
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ PDF")
            return []

        total_chars = sum(len(p) for p in pages_text)
        logger.info(f"‚úì –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(pages_text)} —Å—Ç—Ä–∞–Ω–∏—Ü, –≤—Å–µ–≥–æ {total_chars} —Å–∏–º–≤–æ–ª–æ–≤")
        return pages_text
    except Exception as e:
        logger.error(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ PDF: {e}")
        return []


def split_page_into_paragraphs(page_text: str) -> List[str]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –∞–±–∑–∞—Ü—ã"""
    paragraphs = re.split(r'\n\n+', page_text)

    valid_paragraphs = []
    for para in paragraphs:
        para = para.strip()
        if len(para) > 100:
            valid_paragraphs.append(para)

    return valid_paragraphs


def create_chunks_from_pages(pages_text: List[str]) -> List[str]:
    """–°–æ–∑–¥–∞—ë—Ç –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫—É—Å–∫–∏ –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –∞–±–∑–∞—Ü–µ–≤"""
    chunks = []

    for page_num, page_text in enumerate(pages_text):
        paragraphs = split_page_into_paragraphs(page_text)

        if not paragraphs:
            continue

        i = 0
        while i < len(paragraphs):
            chunk = paragraphs[i]

            if i + 1 < len(paragraphs):
                combined = chunk + "\n\n" + paragraphs[i + 1]
                if len(combined) < 2000:
                    chunk = combined
                    i += 1

            if i + 1 < len(paragraphs):
                combined = chunk + "\n\n" + paragraphs[i + 1]
                if len(combined) < 2500:
                    chunk = combined
                    i += 1

            if chunk and len(chunk) > 100:
                chunks.append(chunk)

            i += 1

    logger.info(f"‚úì –°–æ–∑–¥–∞–Ω–æ {len(chunks)} –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
    return chunks


def extract_sentences_as_candidates(text: str) -> List[Tuple[str, float]]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∫–∞–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤
    """
    if not text or len(text.strip()) < 50:
        return []

    try:
        sentences = sent_tokenize(text)
    except:
        sentences = re.split(r'[.!?]', text)

    candidates = []

    for sentence in sentences:
        sentence = sentence.strip()

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        if len(sentence) < 20:
            continue

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è
        if not re.search(r'[–∞-—è–ê-–Ø]', sentence):
            continue

        # –í—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–º–µ—é—Ç –±–∞–∑–æ–≤—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        score = 0.7

        # –î–ª–∏–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã
        if len(sentence) > 100:
            score = 0.85
        elif len(sentence) > 60:
            score = 0.8

        candidates.append((sentence, score))

    # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    return candidates[:4]


def clean_generated_question(raw_text: str) -> str:
    """
    –¢—â–∞—Ç–µ–ª—å–Ω–æ –æ—á–∏—â–∞–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
    """
    if not raw_text:
        return None

    text = str(raw_text).strip()

    # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã T5
    text = re.sub(r'<extra_id_\d+>', '', text)
    text = re.sub(r'</s>|<s>|<pad>|<unk>|<mask>', '', text)

    # –£–±–∏—Ä–∞–µ–º —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ –∫–æ–º–∞–Ω–¥—ã
    text = re.sub(r'generate\s+question:?\s*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'question:?\s*', '', text, flags=re.IGNORECASE)

    # –£–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä –∏–∑ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã)
    text = re.sub(r'([–∞-—è—ë])\1{2,}', r'\1', text)  # –∞–∞–∞–∞ -> –∞

    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'^[^\w\u0400-\u04FF]+', '', text)  # –ú—É—Å–æ—Ä –≤ –Ω–∞—á–∞–ª–µ
    text = re.sub(r'[^\w\u0400-\u04FF\.!?—ë]+$', '', text)  # –ú—É—Å–æ—Ä –≤ –∫–æ–Ω—Ü–µ

    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –ø—É—Å—Ç–æ–π
    if not text or len(text) < 5:
        return None

    # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–æ–º
    text = text.rstrip('.!,;:')
    if not text.endswith('?'):
        text = text + '?'

    logger.debug(f"Cleaned: {text}")
    return text


def generate_question_from_context(context: str) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É—è fine-tuned T5 –º–æ–¥–µ–ª—å
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç input/output
    """

    if not qg_model or not qg_tokenizer:
        logger.warning("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        return None

    try:
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º input - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–∞–∫ –µ—Å—Ç—å
        input_text = context[:500].strip()

        if not input_text:
            return None

        logger.debug(f"Input text: {input_text[:100]}...")

        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        inputs = qg_tokenizer(
            input_text,
            max_length=512,
            truncation=True,
            padding="longest",
            return_tensors="pt"
        )

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–∞ —Ç–æ–º –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ —á—Ç–æ –∏ –º–æ–¥–µ–ª—å
        device = next(qg_model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º beam search –¥–ª—è –±–æ–ª–µ–µ —Ö–æ—Ä–æ—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
            output_ids = qg_model.generate(
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                max_length=100,
                min_length=10,
                num_beams=5,
                temperature=0.7,
                do_sample=False,
                early_stopping=True,
                no_repeat_ngram_size=2,  # –ò–∑–±–µ–≥–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
                length_penalty=1.0
            )

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Å skip_special_tokens=True
        raw_question = qg_tokenizer.decode(
            output_ids[0],
            skip_special_tokens=True
        ).strip()

        logger.debug(f"Raw output: {raw_question}")

        # –û—á–∏—â–∞–µ–º
        question = clean_generated_question(raw_question)

        if question and len(question) > 7:
            logger.debug(f"‚úì Final question: {question}")
            return question

        logger.debug(f"‚ùå Question too short after cleaning")
        return None

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–∞: {e}")
        import traceback
        traceback.print_exc()
        return None


def generate_qa_from_text_neural(text: str, num_pairs: int = 2) -> List[Dict]:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è QA –ø–∞—Ä –∏—Å–ø–æ–ª—å–∑—É—è fine-tuned neural –º–æ–¥–µ–ª—å
    """

    if not text or len(text.strip()) < 100:
        return []

    qa_pairs = []

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    candidates = extract_sentences_as_candidates(text)

    if not candidates:
        logger.debug("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
        return []

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    for context, relevance in candidates[:num_pairs]:
        try:
            question = generate_question_from_context(context)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å
            if (question and
                len(question) > 7 and
                question.endswith('?') and
                question.lower() != context.lower()[:len(question)]):

                qa_pairs.append({
                    "question": question,
                    "answer": context,
                    "confidence": round(float(relevance), 3)
                })
                logger.debug(f"‚úì Valid pair created")
            else:
                if not question:
                    logger.debug(f"‚ö†Ô∏è No question generated")
                elif not question.endswith('?'):
                    logger.debug(f"‚ö†Ô∏è Question doesn't end with ?")
                else:
                    logger.debug(f"‚ö†Ô∏è Question matches answer")

        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ QA: {e}")
            continue

    return qa_pairs


def process_pdf(pdf_path: str, max_cards: int = 10) -> List[Dict]:
    """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç PDF –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–∞—Ä—Ç–æ—á–∫–∏"""

    logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ PDF: {pdf_path}")

    pages_text = extract_text_from_pdf(pdf_path)
    if not pages_text:
        return []

    chunks = create_chunks_from_pages(pages_text)
    if not chunks:
        return []

    logger.info(f"üìö –í—Å–µ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(chunks)}")

    flashcards = []

    for i, chunk in enumerate(chunks):
        if len(flashcards) >= max_cards:
            logger.info(f"‚úì –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–∞—Ä—Ç–æ—á–µ–∫ ({len(flashcards)}/{max_cards})")
            break

        if (i + 1) % 5 == 0:
            logger.info(f"üìù –§—Ä–∞–≥–º–µ–Ω—Ç {i + 1}/{len(chunks)}... (–∫–∞—Ä—Ç–æ—á–µ–∫: {len(flashcards)})")

        qa_pairs = generate_qa_from_text_neural(chunk, num_pairs=2)

        for qa in qa_pairs:
            if len(flashcards) < max_cards:
                flashcards.append({
                    "question": qa["question"],
                    "answer": qa["answer"],
                    "context": chunk[:300],
                    "confidence": qa["confidence"],
                    "source": pdf_path
                })

    logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(flashcards)} –∫–∞—Ä—Ç–æ—á–µ–∫ –∏–∑ {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
    return flashcards


class QAPair:
    """–ö–ª–∞—Å—Å QA –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ - –æ—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å"""

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç fine-tuned –º–æ–¥–µ–ª—å"""
        logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é Neural QA –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä...")
        load_qg_model()

    def process_pdf(self, pdf_path: str, max_cards: int = 10) -> List[Dict]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç PDF –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–∞—Ä—Ç–æ—á–µ–∫"""
        return process_pdf(pdf_path, max_cards)

    def generate_qa(self, text: str) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç QA –ø–∞—Ä—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        return generate_qa_from_text_neural(text)